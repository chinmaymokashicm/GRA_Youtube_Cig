{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.parsing.preprocessing\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "import os\n",
    "# nltk.download('wordnet')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "dir = \"generated_tables\"\n",
    "df_video = pd.read_csv(os.path.join(dir, \"video_info.csv\"))\n",
    "df_video[\"text\"] = df_video[\"video_title\"] + \"\\n\\n\" + df_video[\"video_description\"]\n",
    "df_video = df_video.rename(columns={\"video_url\": \"url\"})\n",
    "df_video_short = df_video[[\"url\", \"text\"]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": nltk.corpus.wordnet.ADJ,\n",
    "                \"N\": nltk.corpus.wordnet.NOUN,\n",
    "                \"V\": nltk.corpus.wordnet.VERB,\n",
    "                \"R\": nltk.corpus.wordnet.ADV}\n",
    "    return(tag_dict.get(tag, nltk.corpus.wordnet.NOUN))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    try:\n",
    "        list_sentences = nltk.sent_tokenize(text)\n",
    "        list_words = []\n",
    "        for sentence in list_sentences:\n",
    "            list_words = list_words + [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)]\n",
    "        return(list_words)\n",
    "    except Exception:\n",
    "        return([])\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_word(list_words):\n",
    "    try:\n",
    "        list_stems = [stemmer.stem(word) for word in list_words]\n",
    "        return(list_stems)\n",
    "    except Exception:\n",
    "        return([])\n",
    "\n",
    "def pre_process(list_words):\n",
    "    try:\n",
    "        list_words = [word for word in list_words if(len(word) > 2 and word not in gensim.parsing.preprocessing.STOPWORDS)]\n",
    "        return(list_words)\n",
    "    except Exception:\n",
    "        return([])\n",
    "\n",
    "# Generate lemmatized words\n",
    "df_video_short[\"words\"] = df_video_short.text.apply(lemmatize_text)\n",
    "df_video_short.words = df_video_short.words.apply(stem_word)\n",
    "df_video_short.words = df_video_short.words.apply(pre_process)\n",
    "df_video_short.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-3-ffbc4e8d6868>:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_video_short[\"words\"] = df_video_short.text.apply(lemmatize_text)\n",
      "/usr/lib/python3/dist-packages/pandas/core/generic.py:5208: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                       url  \\\n",
       "0  https://youtube.com/watch?v=LEUaxC_QqWE   \n",
       "1  https://youtube.com/watch?v=7NDfG7Pdzhg   \n",
       "2  https://youtube.com/watch?v=wTZL0xYyaBs   \n",
       "3  https://youtube.com/watch?v=pm_9MF7YbmA   \n",
       "4  https://youtube.com/watch?v=P0RYJIJd0V8   \n",
       "\n",
       "                                                text  \\\n",
       "0  #vAustinL Vape Trick Tutorial -  How to Vape O...   \n",
       "1  $100 Vape vs $5 Vape\\n\\nExpensive box mod vs a...   \n",
       "2  New Studio Vape Trick Session\\n\\nCatch me live...   \n",
       "3  CBDfx Terpene CBD Vape Pen Review - To Be Hone...   \n",
       "4  Best Satisfying Vape Tricks 2018 !!! Special T...   \n",
       "\n",
       "                                               words  \n",
       "0  [vaustinl, vape, trick, tutori, vape, ring, ne...  \n",
       "1  [100, vape, vape, expens, box, mod, regular, d...  \n",
       "2  [new, studio, vape, trick, session, catch, liv...  \n",
       "3  [cbdfx, terpen, cbd, vape, pen, review, honest...  \n",
       "4  [best, satisfi, vape, trick, 2018, special, ta...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://youtube.com/watch?v=LEUaxC_QqWE</td>\n",
       "      <td>#vAustinL Vape Trick Tutorial -  How to Vape O...</td>\n",
       "      <td>[vaustinl, vape, trick, tutori, vape, ring, ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://youtube.com/watch?v=7NDfG7Pdzhg</td>\n",
       "      <td>$100 Vape vs $5 Vape\\n\\nExpensive box mod vs a...</td>\n",
       "      <td>[100, vape, vape, expens, box, mod, regular, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://youtube.com/watch?v=wTZL0xYyaBs</td>\n",
       "      <td>New Studio Vape Trick Session\\n\\nCatch me live...</td>\n",
       "      <td>[new, studio, vape, trick, session, catch, liv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://youtube.com/watch?v=pm_9MF7YbmA</td>\n",
       "      <td>CBDfx Terpene CBD Vape Pen Review - To Be Hone...</td>\n",
       "      <td>[cbdfx, terpen, cbd, vape, pen, review, honest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://youtube.com/watch?v=P0RYJIJd0V8</td>\n",
       "      <td>Best Satisfying Vape Tricks 2018 !!! Special T...</td>\n",
       "      <td>[best, satisfi, vape, trick, 2018, special, ta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "dict_words = gensim.corpora.Dictionary(df_video_short.words)\n",
    "dict_words.filter_extremes(no_below=10, no_above=0.5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "list_bow_corpus = [dict_words.doc2bow(doc) for doc in df_video_short.words]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "model_tfidf = gensim.models.TfidfModel(list_bow_corpus)\n",
    "corpus_tfidf = model_tfidf[list_bow_corpus]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "model_lda = gensim.models.LdaMulticore(list_bow_corpus, num_topics=10, id2word=dict_words, passes=2)\n",
    "for idx, topic in model_lda.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Topic: 0 \n",
      "Words: 0.042*\"video\" + 0.034*\"quit\" + 0.028*\"smoke\" + 0.028*\"thi\" + 0.026*\"nicotin\" + 0.018*\"cigarett\" + 0.016*\"e-cigarett\" + 0.013*\"caus\" + 0.011*\"work\" + 0.010*\"addict\"\n",
      "Topic: 1 \n",
      "Words: 0.050*\"smoke\" + 0.030*\"video\" + 0.028*\"thi\" + 0.022*\"trick\" + 0.021*\"nicotin\" + 0.020*\"use\" + 0.015*\"like\" + 0.014*\"quit\" + 0.013*\"pen\" + 0.013*\"best\"\n",
      "Topic: 2 \n",
      "Words: 0.048*\"e-cigarett\" + 0.035*\"cigarett\" + 0.034*\"smoke\" + 0.028*\"use\" + 0.021*\"thi\" + 0.016*\"new\" + 0.014*\"electron\" + 0.014*\"medic\" + 0.013*\"coil\" + 0.012*\"health\"\n",
      "Topic: 3 \n",
      "Words: 0.025*\"today\" + 0.023*\"subscrib\" + 0.021*\"instagram\" + 0.020*\"twitter\" + 0.020*\"facebook\" + 0.018*\"smoke\" + 0.018*\"cbd\" + 0.018*\"use\" + 0.016*\"follow\" + 0.015*\"trick\"\n",
      "Topic: 4 \n",
      "Words: 0.046*\"video\" + 0.031*\"new\" + 0.027*\"e-cigarett\" + 0.020*\"visit\" + 0.017*\"page\" + 0.017*\"facebook\" + 0.017*\"subscrib\" + 0.015*\"follow\" + 0.014*\"medic\" + 0.014*\"twitter\"\n",
      "Topic: 5 \n",
      "Words: 0.062*\"review\" + 0.051*\"pen\" + 0.029*\"thi\" + 0.025*\"video\" + 0.018*\"best\" + 0.017*\"dispos\" + 0.016*\"vapor\" + 0.015*\"kit\" + 0.015*\"instagram\" + 0.013*\"product\"\n",
      "Topic: 6 \n",
      "Words: 0.049*\"pen\" + 0.033*\"thi\" + 0.031*\"product\" + 0.027*\"use\" + 0.023*\"video\" + 0.019*\"nicotin\" + 0.017*\"smoke\" + 0.014*\"review\" + 0.013*\"smok\" + 0.012*\"instagram\"\n",
      "Topic: 7 \n",
      "Words: 0.050*\"thi\" + 0.040*\"pen\" + 0.038*\"video\" + 0.034*\"batteri\" + 0.019*\"comment\" + 0.016*\"cartridg\" + 0.016*\"best\" + 0.015*\"ani\" + 0.015*\"510\" + 0.013*\"question\"\n",
      "Topic: 8 \n",
      "Words: 0.059*\"trick\" + 0.043*\"video\" + 0.027*\"thi\" + 0.022*\"dr.\" + 0.018*\"subscrib\" + 0.017*\"compil\" + 0.016*\"guy\" + 0.015*\"like\" + 0.015*\"instagram\" + 0.014*\"new\"\n",
      "Topic: 9 \n",
      "Words: 0.040*\"thi\" + 0.030*\"smoke\" + 0.029*\"e-cigarett\" + 0.023*\"health\" + 0.017*\"video\" + 0.016*\"lung\" + 0.015*\"new\" + 0.013*\"tobacco\" + 0.012*\"follow\" + 0.012*\"use\"\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "model_lda_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dict_words, passes=2)\n",
    "for idx, topic in model_lda_tfidf.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Topic: 0 \n",
      "Words: 0.028*\"trick\" + 0.012*\"click\" + 0.011*\"compil\" + 0.010*\"tutori\" + 0.010*\"guy\" + 0.010*\"color\" + 0.010*\"page\" + 0.009*\"video\" + 0.009*\"enjoy\" + 0.008*\"comment\"\n",
      "Topic: 1 \n",
      "Words: 0.022*\"pen\" + 0.017*\"batteri\" + 0.016*\"review\" + 0.010*\"electron\" + 0.010*\"thi\" + 0.010*\"code\" + 0.009*\"mod\" + 0.009*\"cartridg\" + 0.009*\"video\" + 0.008*\"cigarett\"\n",
      "Topic: 2 \n",
      "Words: 0.020*\"cigarett\" + 0.014*\"electron\" + 0.012*\"smoke\" + 0.011*\"nicotin\" + 0.011*\"e-cigarett\" + 0.010*\"review\" + 0.010*\"new\" + 0.009*\"morn\" + 0.009*\"devic\" + 0.009*\"juul\"\n",
      "Topic: 3 \n",
      "Words: 0.021*\"e-cigarett\" + 0.013*\"review\" + 0.013*\"today\" + 0.010*\"teen\" + 0.009*\"use\" + 0.009*\"expert\" + 0.009*\"thi\" + 0.008*\"video\" + 0.008*\"public\" + 0.008*\"health\"\n",
      "Topic: 4 \n",
      "Words: 0.017*\"smoke\" + 0.013*\"trick\" + 0.012*\"level\" + 0.010*\"let\" + 0.009*\"group\" + 0.009*\"know\" + 0.009*\"e-cigarett\" + 0.008*\"video\" + 0.008*\"look\" + 0.008*\"anoth\"\n",
      "Topic: 5 \n",
      "Words: 0.021*\"trick\" + 0.012*\"vapetrick\" + 0.011*\"beginn\" + 0.011*\"pod\" + 0.011*\"review\" + 0.010*\"smoke\" + 0.009*\"follow\" + 0.009*\"instagram\" + 0.009*\"tutori\" + 0.008*\"guid\"\n",
      "Topic: 6 \n",
      "Words: 0.014*\"smoke\" + 0.013*\"risk\" + 0.011*\"trick\" + 0.011*\"health\" + 0.010*\"guy\" + 0.009*\"video\" + 0.009*\"thi\" + 0.009*\"cigarett\" + 0.009*\"episod\" + 0.009*\"pen\"\n",
      "Topic: 7 \n",
      "Words: 0.016*\"product\" + 0.016*\"pen\" + 0.014*\"cbd\" + 0.010*\"review\" + 0.010*\"school\" + 0.009*\"thi\" + 0.009*\"e-cigarett\" + 0.009*\"inform\" + 0.008*\"twitter\" + 0.008*\"video\"\n",
      "Topic: 8 \n",
      "Words: 0.020*\"smoke\" + 0.015*\"dr.\" + 0.015*\"vapor\" + 0.015*\"pen\" + 0.013*\"kit\" + 0.011*\"e-cigarett\" + 0.010*\"new\" + 0.010*\"tobacco\" + 0.010*\"use\" + 0.009*\"cigarett\"\n",
      "Topic: 9 \n",
      "Words: 0.021*\"trick\" + 0.012*\"quit\" + 0.011*\"ani\" + 0.011*\"guy\" + 0.011*\"page\" + 0.011*\"new\" + 0.010*\"like\" + 0.010*\"doctor\" + 0.009*\"video\" + 0.009*\"nicotin\"\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}